# RISE RP005 QEMU weekly report 2024-08-28

We have fewer updates this week due to vacations.

## Work completed since last report

- WP2
  - Replicate Nathan's results:
    - **In Progress** Initial results and analysis; and
	- the results show that load/store functions are important in `memcpy`.
  - Profile SPEC CPU 2017 using optimized QEMU:
    - initial result for `625.x264_s`; and
	- Key finding is that vector load store still dominates.
- Other
  - Max Chou's talk was accepted for RISC-V NA Summit.
    - Congratulations to Max
	- Max has invited Jeremy Bennett to be co-author

## Work planned for the coming two weeks

- WP2
  - Further investigation using Linux perf to identify key optimizations.
  - Check the `__builtin_memcpy` for endianness and check its support on Aarch64.
  - Investigate the GCC tuning needed to enable AVX2 instructions to be materialized for `__builtin_memcpy` and then measure performance.

- WP3
  - Investigate the anomalous performance of QEMU on AArch64.

# Detailed description of work

## WP2

All the data used here is in a [Google spreadsheet](https://docs.google.com/spreadsheets/d/1Wg50RgQk2lPIWluirn3bvlVfKTxYumLtOSC06m74CHg).

### Analysis of the bionic benchmark

This benchmark is almost identical to the benchmark we have used from the start of the project.  The implementation of `memcpy` is almost identical to the reference implementation in the RVV standard, with two changes (aside from cosmetic layout):

- the instructions decrementing the count and incrementing the pointer are swapped (they are independent); and
- the choice of registers is different.

The two are shown here side by side

```
bionic_memcpy:                           vmemcpy:
        mv      a4, a0                           mv      a3, a0
loop:                                    loop_cpy:
        vsetvli a3, a2, e8, LMUL, ta, ma         vsetvli t0, a2, e8, LMUL, ta, ma
        vle8.v  v0, (a1)                         vle8.v  v0, (a1)
        sub     a2, a2, a3                       add     a1, a1, t0
        add     a1, a1, a3                       sub     a2, a2, t0
        vse8.v  v0, (a4)                         vse8.v  v0, (a3)
        add     a4, a4, a3                       add     a3, a3, t0
        bnez    a2, loop                         bnez    a2, loop_cpy
        ret                                      ret
```

### Performance profiling of memory benchmark

We have used Linux _perf_ to analyze QEMU running the memory benchmark under a range of scenarios.  To record the profile (in this case for VLEN=1024, LMUL=8 and a block size for the copy of 8192) we used:
```
perf record -g qemu-riscv64 -cpu rv64,v=true,vlen=1024 vmemcpy8.exe 8192 10000
```
Unlike with our regular benchmarks, the number of iterations varies, with larger values used with the smaller block sizes, to ensure we get sufficient samples from _perf_ to be statistically significant.

The data was then recorded as a tree using:
```
perf report -k /tmp/vmlinux
```

**Note.** Collection of data on Linux systems is usually disabled by default.  To resolve kernel symbols, an uncompressed kernel image is required.  So to enabled collection with kernel symbols on Ubuntu 24.04 we used:
```
# /usr/src/linux-headers-5.15.0-58/scripts/extract-vmlinux /vmlinuz \
    > /tmp/vmlinux
# echo -1 > /proc/sys/kernel/perf_event_paranoid
# echo 0 > /proc/sys/kernel/kptr_restrict
```

Scripts were then used to break out the key data.  The results show child data, so we can see how execution time partitions.  For example running vector `memcpy` with VLEN=128 and LMUL=1 and a data block size of 1 byte, we see
```
    99.98%     0.00%  qemu-riscv64  [unknown]                [.] 0x000000000001095a
            |
            ---0x1095a
               |
               |--32.82%--0x7f77980a212b
               |          |
               |          |--31.41%--helper_vse8_v
               |          |          |
               |          |          |--26.31%--vext_ldst_us
               |          |          |          |
               |          |          |          |--5.98%--__resolv_conf_allocate
               |          |          |          |
               |          |          |          |--4.96%--cpu_stb_data_ra
               |          |          |          |          |
...
```
This is a sampling technique, so totals do not necessarily add up, and can vary between runs.  However we can see that almost all the time is in the main command, `qemu-riscv64`.  There are intermediate (likely system) functions known only by their address, but we see that nearly a third of the time is spent in the helper function `helper_vse8_v`, which in turn breaks down into time spent in the central `vect_ldst_us` function and so on.

We need to do more work to understand why so much time is spent in the _Glibc_ `__resolve_conf` functions.  These are part of the subsystem concerned with Internet name resolution.  There is either a problem with mapping addresses to function names, or these functions are reused for memory allocation.

For contrast, this is the data running vector `memcpy` with VLEN=1024, LMUL=8 and a data block size of 9,103 bytes.
```
    99.95%     0.00%  qemu-riscv64  [unknown]                [.] 0x000000000001095a
            |
            ---0x1095a
               |
               |--45.61%--0x7f3ca40a25eb
               |          |
               |           --45.60%--helper_vse8_v
               |                     |
               |                     |--42.65%--vext_ldst_us
               |                     |          |
               |                     |          |--11.94%--cpu_stb_data_ra
               |                     |          |          |
               |                     |          |          |--4.60%--__resolv_conf_allocate
               |                     |          |          |
...
```

We have summarized the results graphically.  First, we look at how the percentage of time spent in the main load/store function, ` vary for the optimized QEMU over different data block sizes.

![`vect_ldst_us` profiled time share](./images/20240828-vect_ldst_us-share.svg)

The point to note here is that for large data sizes, this function dominates, and any optimization to `vect_ldst_us` will pay off directly.  However we already do very well for big data sizes (up to 60x speedup).  Our problem is with small data sizes, and there, while there is a gain worth having, we are at most going to double performance (currently it has no gain).  So we need to look at the other functions involved.

Secondly, we look at how the percentages vary between optimized and unoptimized QEMU.  We have considered two scenarios:
1. "Small" - VLEN = 128, LMUL = 1; and
2. "Large" - VLEN = 1024, LMUL = 8; and

![`vect_ldst_us` profiled time share optimized and unoptimized - "small" scenario](./images/20240828-vect_ldst_us-optimization-small.svg)

![`vect_ldst_us` profiled time share optimized and unoptimized - "large" scenario](./images/20240828-vect_ldst_us-optimization-large.svg)

There is no significant difference.  We think the issue is that, particularly for large data sizes, almost all the computation time is in `vect_ldst_us`, so there is not much scope for the amount to change.

For small data sizes, there is only minimal speed up, so again there is little change.

The problem may be due to using percentages when profiling.  We will also look at samples to see how this varies.

### Performance profiling of `625.x264_s`

We have chosen one of the SPEC CPU 2017 benchmarks which shows a big benefit from the vector `memcpy` optimization.  The goal is to understand the potential for further gains from optimizing `memcpy`.  The results were completely unexpected.  This is from running on the default QEMU vector configuration (VLEN=128)
```
    56.76%     8.83%  qemu-riscv64  qemu-riscv64             [.] vext_ldst_us
            |
            |--47.93%--vext_ldst_us
            |          |
            |          |--29.53%--lde_b
            |          |          |
            |          |          |--20.00%--cpu_ldsb_data_ra
            |          |          |          |
            |          |          |          |--6.90%--__resolv_conf_allocate
            |          |          |          |
            |          |          |          |--4.71%--__resolv_context_get_preinit
            |          |          |          |
            |          |          |          |--2.30%--qemu_plugin_vcpu_mem_cb
            |          |          |          |
            |          |          |           --1.93%--cpu_mmu_lookup
            |          |          |
            |          |          |--3.78%--__resolv_conf_allocate
            |          |          |
            |          |          |--2.22%--__resolv_context_get_preinit
            |          |          |
            |          |          |--1.51%--qemu_plugin_vcpu_mem_cb
            |          |          |
            |          |           --0.86%--cpu_mmu_lookup
```
`vext_ldst_us` accounts for half the sampled execution time.  If we look just at the top level functions, we see (showing any function cumulatively accounting for > 5% of execution):

|  Total |   Self | Function                       |
|-------:|-------:|:-------------------------------|
| 56.76% |  8.83% | `vext_ldst_us`                 |
| 43.17% |  0.35% | `helper_vle8_v`                |
| 31.49% |  2.45% | `lde_b`                        |
| 30.97% | 24.55% | `__resolv_conf_allocate`       |
| 21.99% |  5.31% | `cpu_ldsb_data_ra`             |
| 21.62% | 16.76% | `__resolv_context_get_preinit` |
|  9.15% |  4.55% | `do_vext_vv`                   |
|  7.85% |  0.00% | `0x0000000000060100`           |
|  7.55% |  0.00% | `0x0000000000063222`           |
|  7.37% |  0.00% | `0x00000000000632ec`           |
|  7.35% |  0.00% | `0x0000000000062870`           |
|  7.31% |  0.04% | `helper_vse8_v`                |
|  7.24% |  1.63% | `helper_lookup_tb_ptr`         |
|  5.92% |  5.37% | `qemu_plugin_vcpu_mem_cb`      |
|  5.69% |  0.00% | `0x00000000000622b6`           |
|  5.55% |  0.00% | `0x0000000000062318`           |
|  5.09% |  4.86% | `cpu_mmu_lookup`               |
|  5.02% |  0.18% | `helper_vwsubu_vv_b`           |

This confirms that focusing on `vext_ldst_us` is important.  It is also important to understand why the Glibc resolver code is so prominent.

# Statistics

There are no new statistics, beyond those reported above.

# Actions

2024-08-14
- **Paolo** Run Nathan's memcpy benchmark.
  - **COMPLETE** See above.
- **Jeremy** to put Embecosm's `memcpy` benchmark scripts in the public repository.
  - **COMPLETE** See commit [#5174e91](https://github.com/embecosm/rise-rvv-tcg-qemu-tooling/commit/5174e91bb1e19192c2f6fe2c92003f72eecc9adc) in the [rise-rvv-tcg-qemu-tooling](https://github.com/embecosm/rise-rvv-tcg-qemu-tooling) repository.  This includes the ability to run the Bionic variant.

2024-07-31
- **Max** Add his SPEC CPU benchmark scripts to the above mentioned repository, so Embecosm can reproduce his results.
- **Paolo** Create a reference set of statically linked SPEC CPU 2017 binaries to be made available to those who need them.
  - **ON HOLD**. Lower priority.
- **Paolo** Set up full system emulation. Just timing Linux boot may be sufficient to start with.
  - **ON HOLD** lower priority.

2024-07-17
- **Paolo** Account for regular checks on comments and time required to address feedback on the patch.
  - Review work in progress

2024-06-05
- **Paolo** Check behavior of QEMU with tail bytes.
  - Deferred to prioritize host targeted optimization work.

2024-05-15

- **Jeremy** to look at impact of masked v unmasked and strided v unstrided on vector operations.
  - lower priority.

2024-05-08

- **Jeremy** to characterize QEMU floating point performance and file it as a performance regression issue in QEMU GitLab.
  - low priority, deferred to prioritize the smoke tests work.

# Risk register

The risk register is held in a shared [spreadsheet](https://docs.google.com/spreadsheets/d/1mHNwGGGPJ-ls0pgCbvkSdGDoKW4vftzYWeIPPYZYfjY/edit?usp=sharing), which is updated continuously.

There are no changes to the risk register this week.

# Planned absences

- Jeremy Bennett will be away at conferences 12-16 September
  - includes GNU Tools Cauldron 14-16 September
- Paolo Savini will be on vacation 20-24 and 27 September

# For the record

## The project team
- Paolo Savini (Embecosm)
- Hélène Chelin (Embecosm)
- Jeremy Bennett (Embecosm)
- Hugh O'Keeffe (Ashling)
- Nadim Shehayed (Ashling)
- Daniel Barboza (Ventana)

## Current priorities

Our current set of agreed priorities are as follows

- vector load/store ops for x86_64 AVX
- vector load/store ops for AArch64/Neon
- vector integer ALU ops for x86_64 AVX
- vector load/store ops for Intel AVX10

For each of these there will be an analysis phase and an optimization phase, leading to the following set of work packages.
- WP0: Infrastructure
- WP1: Analysis of vector load/store ops on x86_64 AVX
- WP2: Optimization of vector load/store ops on x86_64 AVX
- WP3: Analysis of vector load/store ops on AArch64/Neon
- WP4: Optimization of vector load/store ops on AArch64/Neon
- WP5: Analysis of integer ALU ops on x86_64 AVX
- WP6: Optimization of integer ALU ops on x86_64 AVX
- WP7: Analysis of vector load/store ops on Intel AVX10
- WP8: Optimization of vector load/store ops on Intel AVX10

These priorities can be revised by agreement with RISE during the project.
